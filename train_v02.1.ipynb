{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_v02.1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RtjShreyD/Eng-Mandarin/blob/master/train_v02.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I48m98tYAnhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6b17c6ac-908e-4bb7-ae99-45c33fd55c70"
      },
      "source": [
        "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.3.0.post4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJg8OGXVBdsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "db6b61c4-9fcd-4def-f201-3af11f50b261"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Collecting torch==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6MB 25kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 0.3.0.post4\n",
            "    Uninstalling torch-0.3.0.post4:\n",
            "      Successfully uninstalled torch-0.3.0.post4\n",
            "Successfully installed torch-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YLIWneIB2KJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "45f8ec22-f490-43fa-d704-3b5556c8569b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHD2dwOBCLmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3c01e0e-7ae8-47ae-f2eb-7f482bbde30d"
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 20.1MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 71.3MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 57.6MB/s \n",
            "\u001b[?25hCollecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.3)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.40)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.4)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 63.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 58.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from responses>=0.7->allennlp) (1.12.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.9)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.40)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Building wheels for collected packages: overrides, numpydoc, parsimonious, ftfy, word2number, jsonnet\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp36-none-any.whl size=5600 sha256=46a0ea051af30debf88b32f1550a44bc4501448321cf20f88fa1c4009981ba0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31894 sha256=adfb8c977911b55ac78605ac955e0f7f6df3bbbec070a3e3e9bed234068af037\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=356a188d3e30e00d59f29b2a3a1b4da69b53da4e705c32ddeaec0e6dfd8ae88a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=c3e73b2a2eb09f018008556e78973014504d853b7216ce7b20c98e19b855e7c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=8069c470ac823cf480d87ccd5f7c3d2317a36aaca2cd05e5938946f6199bfaf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320348 sha256=700e5938cfe7c1c9d6a3955a62a456a1458cfde2cf866123b440bfd7dd33b5e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "Successfully built overrides numpydoc parsimonious ftfy word2number jsonnet\n",
            "Installing collected packages: overrides, responses, numpydoc, jsonpickle, pytorch-pretrained-bert, tensorboardX, flaky, unidecode, sentencepiece, pytorch-transformers, flask-cors, parsimonious, ftfy, word2number, conllu, jsonnet, allennlp\n",
            "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.2 overrides-2.7.0 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 sentencepiece-0.1.85 tensorboardX-1.9 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiKHWg-8Cr0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.dataset_readers.seq2seq import Seq2SeqDatasetReader\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
        "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.nn.activations import Activation\n",
        "from allennlp.models.encoder_decoders.simple_seq2seq import SimpleSeq2Seq\n",
        "from allennlp.modules.attention import LinearAttention, BilinearAttention, DotProductAttention\n",
        "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, StackedSelfAttentionEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.predictors import SimpleSeq2SeqPredictor\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "EN_EMBEDDING_DIM = 256\n",
        "ZH_EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT_pNvy6GCeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3ec9afec-eee5-4d89-97f5-b9d76520345f"
      },
      "source": [
        "reader = Seq2SeqDatasetReader(\n",
        "        source_tokenizer=WordTokenizer(),\n",
        "        target_tokenizer=CharacterTokenizer(),\n",
        "        source_token_indexers={'tokens': SingleIdTokenIndexer()},\n",
        "        target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\n",
        "    \n",
        "train_dataset = reader.read('/content/drive/My Drive/Eng_Mandarin/data/tatoeba.eng_cmn.train.tsv')\n",
        "validation_dataset = reader.read('/content/drive/My Drive/Eng_Mandarin/data/tatoeba.eng_cmn.dev.tsv')\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset + validation_dataset, \n",
        "                                  min_count={'tokens': 3, 'target_tokens': 3})\n",
        "\n",
        "en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), \n",
        "                         embedding_dim=EN_EMBEDDING_DIM)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37400it [00:12, 2900.89it/s]\n",
            "4676it [00:01, 3201.16it/s]\n",
            "100%|██████████| 42076/42076 [00:00<00:00, 66525.86it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkKjDzNNGKQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = StackedSelfAttentionEncoder(\n",
        "        input_dim=EN_EMBEDDING_DIM, \n",
        "        hidden_dim=HIDDEN_DIM, \n",
        "        projection_dim=128, \n",
        "        feedforward_hidden_dim=128, \n",
        "        num_layers=1, \n",
        "        num_attention_heads=8)\n",
        "\n",
        "source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBAAOxIyGWjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a558557a-dcae-4684-fbc3-5ba1036ad7e6"
      },
      "source": [
        "attention = DotProductAttention()\n",
        "\n",
        "max_decoding_steps = 20 \n",
        "\n",
        "model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
        "                      target_embedding_dim=ZH_EMBEDDING_DIM,\n",
        "                      target_namespace='target_tokens',\n",
        "                      attention=attention,\n",
        "                      beam_size=8,\n",
        "                      use_bleu=True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  CUDA_DEVICE = 0\n",
        "  model = model.cuda(CUDA_DEVICE)\n",
        "  print(\"Model deployed on GPU\")\n",
        "else:\n",
        "  CUDA_DEVICE = -1\n",
        "  print(\"Model deployed on CPU\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
        "\n",
        "iterator.index_with(vocab)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model deployed on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4yZwOfjvUw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e56efe2-76ee-4770-dd22-21cb67dd502a"
      },
      "source": [
        "mod_path = '/content/drive/My Drive/Eng_Mandarin/data/tmp_v02.1'\n",
        "try:   \n",
        "    if(os.path.exists(mod_path)):\n",
        "      print(\"\\nModel Path exists move ahead\")\n",
        "    else:\n",
        "      os.mkdir(mod_path)\n",
        "except OSError:\n",
        "    print (\"Creation of the directory failed\")\n",
        "else:  \n",
        "    print (\"Successfully created the tmp directory model path\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the tmp directory model path\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ42Pm9XrzZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d3a3d71-b395-4c55-cb98-4d0262d9201e"
      },
      "source": [
        "try:\n",
        "  trainer = Trainer(model=model, \n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset,\n",
        "                  num_epochs=70,\n",
        "                  cuda_device=CUDA_DEVICE)\n",
        "                     \n",
        "  trainer.train()\n",
        "  print(\"*******************************Completed 70 epochs***********************************\")\n",
        "  predictor = SimpleSeq2SeqPredictor(model, reader)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"Training interrupted....all weights saved\")\n",
        "\n",
        "for instance in itertools.islice(validation_dataset, 10):\n",
        "  print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "  print('GOLD:', instance.fields['target_tokens'].tokens)\n",
        "  print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])\n",
        "  print('////////////////////////////////////////////\\n')\n",
        "\n",
        "with open(os.path.join(mod_path, \"model.th\"), 'wb') as f:\n",
        "    torch.save(model.state_dict(), f)\n",
        "vocab.save_to_files(os.path.join(mod_path, \"vocabulary\"))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "loss: 4.6968 ||: 100%|██████████| 1169/1169 [00:39<00:00, 29.69it/s]\n",
            "BLEU: 0.0086, loss: 4.1483 ||: 100%|██████████| 147/147 [00:12<00:00, 12.25it/s]\n",
            "loss: 3.8138 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.77it/s]\n",
            "BLEU: 0.0157, loss: 3.6491 ||: 100%|██████████| 147/147 [00:12<00:00, 11.43it/s]\n",
            "loss: 3.3700 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.72it/s]\n",
            "BLEU: 0.0298, loss: 3.3443 ||: 100%|██████████| 147/147 [00:12<00:00, 11.64it/s]\n",
            "loss: 3.0333 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.70it/s]\n",
            "BLEU: 0.0445, loss: 3.1122 ||: 100%|██████████| 147/147 [00:12<00:00, 11.86it/s]\n",
            "loss: 2.7483 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.85it/s]\n",
            "BLEU: 0.0574, loss: 2.9382 ||: 100%|██████████| 147/147 [00:12<00:00, 11.32it/s]\n",
            "loss: 2.5116 ||: 100%|██████████| 1169/1169 [00:36<00:00, 30.57it/s]\n",
            "BLEU: 0.0725, loss: 2.8033 ||: 100%|██████████| 147/147 [00:13<00:00, 11.15it/s]\n",
            "loss: 2.3182 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.42it/s]\n",
            "BLEU: 0.0860, loss: 2.7085 ||: 100%|██████████| 147/147 [00:13<00:00, 11.31it/s]\n",
            "loss: 2.1557 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.90it/s]\n",
            "BLEU: 0.0918, loss: 2.6442 ||: 100%|██████████| 147/147 [00:12<00:00, 11.63it/s]\n",
            "loss: 2.0143 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.39it/s]\n",
            "BLEU: 0.1027, loss: 2.6006 ||: 100%|██████████| 147/147 [00:12<00:00, 11.61it/s]\n",
            "loss: 1.8893 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.86it/s]\n",
            "BLEU: 0.1106, loss: 2.5614 ||: 100%|██████████| 147/147 [00:13<00:00, 11.06it/s]\n",
            "loss: 1.7789 ||: 100%|██████████| 1169/1169 [00:36<00:00, 34.53it/s]\n",
            "BLEU: 0.1175, loss: 2.5407 ||: 100%|██████████| 147/147 [00:13<00:00, 11.26it/s]\n",
            "loss: 1.6811 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.00it/s]\n",
            "BLEU: 0.1192, loss: 2.5418 ||: 100%|██████████| 147/147 [00:12<00:00, 11.51it/s]\n",
            "loss: 1.5891 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.09it/s]\n",
            "BLEU: 0.1244, loss: 2.5357 ||: 100%|██████████| 147/147 [00:12<00:00, 11.37it/s]\n",
            "loss: 1.5084 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.59it/s]\n",
            "BLEU: 0.1276, loss: 2.5310 ||: 100%|██████████| 147/147 [00:12<00:00, 11.46it/s]\n",
            "loss: 1.4333 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.89it/s]\n",
            "BLEU: 0.1276, loss: 2.5548 ||: 100%|██████████| 147/147 [00:12<00:00, 11.52it/s]\n",
            "loss: 1.3627 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.94it/s]\n",
            "BLEU: 0.1352, loss: 2.5766 ||: 100%|██████████| 147/147 [00:12<00:00, 11.36it/s]\n",
            "loss: 1.2992 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.79it/s]\n",
            "BLEU: 0.1351, loss: 2.5736 ||: 100%|██████████| 147/147 [00:12<00:00,  7.10it/s]\n",
            "loss: 1.2373 ||: 100%|██████████| 1169/1169 [00:36<00:00, 28.29it/s]\n",
            "BLEU: 0.1349, loss: 2.5890 ||: 100%|██████████| 147/147 [00:13<00:00, 11.16it/s]\n",
            "loss: 1.1824 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.95it/s]\n",
            "BLEU: 0.1411, loss: 2.6245 ||: 100%|██████████| 147/147 [00:12<00:00, 11.41it/s]\n",
            "loss: 1.1286 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.85it/s]\n",
            "BLEU: 0.1364, loss: 2.6452 ||: 100%|██████████| 147/147 [00:12<00:00, 11.38it/s]\n",
            "loss: 1.0800 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.31it/s]\n",
            "BLEU: 0.1413, loss: 2.6668 ||: 100%|██████████| 147/147 [00:13<00:00, 11.22it/s]\n",
            "loss: 1.0347 ||: 100%|██████████| 1169/1169 [00:36<00:00, 34.51it/s]\n",
            "BLEU: 0.1395, loss: 2.6980 ||: 100%|██████████| 147/147 [00:13<00:00, 11.22it/s]\n",
            "loss: 0.9898 ||: 100%|██████████| 1169/1169 [00:36<00:00, 35.01it/s]\n",
            "BLEU: 0.1424, loss: 2.7217 ||: 100%|██████████| 147/147 [00:13<00:00, 11.12it/s]\n",
            "loss: 0.9501 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.91it/s]\n",
            "BLEU: 0.1471, loss: 2.7452 ||: 100%|██████████| 147/147 [00:12<00:00, 11.44it/s]\n",
            "loss: 0.9120 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.98it/s]\n",
            "BLEU: 0.1410, loss: 2.7867 ||: 100%|██████████| 147/147 [00:12<00:00,  7.21it/s]\n",
            "loss: 0.8745 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.08it/s]\n",
            "BLEU: 0.1447, loss: 2.8147 ||: 100%|██████████| 147/147 [00:13<00:00, 11.27it/s]\n",
            "loss: 0.8399 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.88it/s]\n",
            "BLEU: 0.1445, loss: 2.8417 ||: 100%|██████████| 147/147 [00:13<00:00, 11.28it/s]\n",
            "loss: 0.8092 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.36it/s]\n",
            "BLEU: 0.1495, loss: 2.8644 ||: 100%|██████████| 147/147 [00:13<00:00,  7.02it/s]\n",
            "loss: 0.7771 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.19it/s]\n",
            "BLEU: 0.1495, loss: 2.9044 ||: 100%|██████████| 147/147 [00:13<00:00, 11.00it/s]\n",
            "loss: 0.7487 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.02it/s]\n",
            "BLEU: 0.1468, loss: 2.9320 ||: 100%|██████████| 147/147 [00:13<00:00, 11.24it/s]\n",
            "loss: 0.7202 ||: 100%|██████████| 1169/1169 [00:36<00:00, 30.81it/s]\n",
            "BLEU: 0.1477, loss: 2.9623 ||: 100%|██████████| 147/147 [00:12<00:00, 11.35it/s]\n",
            "loss: 0.6948 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.93it/s]\n",
            "BLEU: 0.1460, loss: 2.9925 ||: 100%|██████████| 147/147 [00:13<00:00, 11.29it/s]\n",
            "loss: 0.6700 ||: 100%|██████████| 1169/1169 [00:36<00:00, 24.93it/s]\n",
            "BLEU: 0.1458, loss: 3.0343 ||: 100%|██████████| 147/147 [00:12<00:00,  7.27it/s]\n",
            "loss: 0.6473 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.43it/s]\n",
            "BLEU: 0.1487, loss: 3.0656 ||: 100%|██████████| 147/147 [00:13<00:00, 11.02it/s]\n",
            "loss: 0.6256 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.04it/s]\n",
            "BLEU: 0.1453, loss: 3.0977 ||: 100%|██████████| 147/147 [00:12<00:00, 11.34it/s]\n",
            "loss: 0.6027 ||: 100%|██████████| 1169/1169 [00:36<00:00, 29.20it/s]\n",
            "BLEU: 0.1472, loss: 3.1235 ||: 100%|██████████| 147/147 [00:13<00:00, 11.25it/s]\n",
            "loss: 0.5837 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.90it/s]\n",
            "BLEU: 0.1465, loss: 3.1607 ||: 100%|██████████| 147/147 [00:13<00:00, 11.26it/s]\n",
            "loss: 0.5651 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.01it/s]\n",
            "BLEU: 0.1475, loss: 3.1890 ||: 100%|██████████| 147/147 [00:13<00:00, 11.18it/s]\n",
            "loss: 0.5478 ||: 100%|██████████| 1169/1169 [00:36<00:00, 33.64it/s]\n",
            "BLEU: 0.1495, loss: 3.2215 ||: 100%|██████████| 147/147 [00:13<00:00, 11.06it/s]\n",
            "loss: 0.5299 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.89it/s]\n",
            "BLEU: 0.1467, loss: 3.2608 ||: 100%|██████████| 147/147 [00:12<00:00, 11.36it/s]\n",
            "loss: 0.5122 ||: 100%|██████████| 1169/1169 [00:37<00:00, 35.34it/s]\n",
            "BLEU: 0.1496, loss: 3.2923 ||: 100%|██████████| 147/147 [00:13<00:00, 10.99it/s]\n",
            "loss: 0.4975 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.04it/s]\n",
            "BLEU: 0.1486, loss: 3.3134 ||: 100%|██████████| 147/147 [00:13<00:00,  6.96it/s]\n",
            "loss: 0.4835 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.99it/s]\n",
            "BLEU: 0.1467, loss: 3.3354 ||: 100%|██████████| 147/147 [00:13<00:00, 11.11it/s]\n",
            "loss: 0.4699 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.93it/s]\n",
            "BLEU: 0.1428, loss: 3.3833 ||: 100%|██████████| 147/147 [00:13<00:00, 11.28it/s]\n",
            "loss: 0.4567 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.99it/s]\n",
            "BLEU: 0.1499, loss: 3.3997 ||: 100%|██████████| 147/147 [00:13<00:00, 11.05it/s]\n",
            "loss: 0.4450 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.08it/s]\n",
            "BLEU: 0.1489, loss: 3.4431 ||: 100%|██████████| 147/147 [00:13<00:00, 11.30it/s]\n",
            "loss: 0.4317 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.52it/s]\n",
            "BLEU: 0.1452, loss: 3.4663 ||: 100%|██████████| 147/147 [00:12<00:00, 11.35it/s]\n",
            "loss: 0.4193 ||: 100%|██████████| 1169/1169 [00:36<00:00, 36.21it/s]\n",
            "BLEU: 0.1486, loss: 3.4995 ||: 100%|██████████| 147/147 [00:12<00:00, 11.31it/s]\n",
            "loss: 0.4105 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.35it/s]\n",
            "BLEU: 0.1489, loss: 3.5274 ||: 100%|██████████| 147/147 [00:13<00:00, 11.30it/s]\n",
            "loss: 0.3962 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.33it/s]\n",
            "BLEU: 0.1465, loss: 3.5601 ||: 100%|██████████| 147/147 [00:13<00:00, 11.18it/s]\n",
            "loss: 0.3873 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.44it/s]\n",
            "BLEU: 0.1471, loss: 3.6022 ||: 100%|██████████| 147/147 [00:12<00:00, 11.32it/s]\n",
            "loss: 0.3775 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.44it/s]\n",
            "BLEU: 0.1450, loss: 3.6166 ||: 100%|██████████| 147/147 [00:13<00:00, 11.00it/s]\n",
            "loss: 0.3677 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.35it/s]\n",
            "BLEU: 0.1445, loss: 3.6548 ||: 100%|██████████| 147/147 [00:13<00:00, 11.24it/s]\n",
            "loss: 0.3600 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.91it/s]\n",
            "BLEU: 0.1466, loss: 3.6822 ||: 100%|██████████| 147/147 [00:13<00:00, 11.29it/s]\n",
            "loss: 0.3536 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.45it/s]\n",
            "BLEU: 0.1458, loss: 3.7078 ||: 100%|██████████| 147/147 [00:13<00:00, 11.24it/s]\n",
            "loss: 0.3443 ||: 100%|██████████| 1169/1169 [00:35<00:00, 32.57it/s]\n",
            "BLEU: 0.1448, loss: 3.7409 ||: 100%|██████████| 147/147 [00:13<00:00, 11.26it/s]\n",
            "loss: 0.3350 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.32it/s]\n",
            "BLEU: 0.1470, loss: 3.7533 ||: 100%|██████████| 147/147 [00:12<00:00, 11.37it/s]\n",
            "loss: 0.3298 ||: 100%|██████████| 1169/1169 [00:35<00:00, 32.56it/s]\n",
            "BLEU: 0.1466, loss: 3.7826 ||: 100%|██████████| 147/147 [00:13<00:00, 11.27it/s]\n",
            "loss: 0.3196 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.37it/s]\n",
            "BLEU: 0.1457, loss: 3.8177 ||: 100%|██████████| 147/147 [00:12<00:00, 11.43it/s]\n",
            "loss: 0.3176 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.09it/s]\n",
            "BLEU: 0.1465, loss: 3.8439 ||: 100%|██████████| 147/147 [00:13<00:00, 11.21it/s]\n",
            "loss: 0.3073 ||: 100%|██████████| 1169/1169 [00:36<00:00, 30.14it/s]\n",
            "BLEU: 0.1439, loss: 3.8614 ||: 100%|██████████| 147/147 [00:12<00:00, 11.32it/s]\n",
            "loss: 0.3007 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.40it/s]\n",
            "BLEU: 0.1428, loss: 3.8968 ||: 100%|██████████| 147/147 [00:12<00:00, 11.33it/s]\n",
            "loss: 0.2974 ||: 100%|██████████| 1169/1169 [00:38<00:00, 30.55it/s]\n",
            "BLEU: 0.1448, loss: 3.9258 ||: 100%|██████████| 147/147 [00:13<00:00, 11.13it/s]\n",
            "loss: 0.2899 ||: 100%|██████████| 1169/1169 [00:36<00:00, 35.42it/s]\n",
            "BLEU: 0.1457, loss: 3.9289 ||: 100%|██████████| 147/147 [00:13<00:00, 11.12it/s]\n",
            "loss: 0.2871 ||: 100%|██████████| 1169/1169 [00:36<00:00, 26.87it/s]\n",
            "BLEU: 0.1454, loss: 3.9838 ||: 100%|██████████| 147/147 [00:13<00:00, 10.96it/s]\n",
            "loss: 0.2781 ||: 100%|██████████| 1169/1169 [00:36<00:00, 30.73it/s]\n",
            "BLEU: 0.1481, loss: 3.9903 ||: 100%|██████████| 147/147 [00:13<00:00, 11.08it/s]\n",
            "loss: 0.2728 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.24it/s]\n",
            "BLEU: 0.1448, loss: 4.0199 ||: 100%|██████████| 147/147 [00:13<00:00, 11.00it/s]\n",
            "loss: 0.2724 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.79it/s]\n",
            "BLEU: 0.1458, loss: 4.0757 ||: 100%|██████████| 147/147 [00:13<00:00, 11.12it/s]\n",
            "loss: 0.2634 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.75it/s]\n",
            "BLEU: 0.1480, loss: 4.0646 ||: 100%|██████████| 147/147 [00:13<00:00, 11.15it/s]\n",
            "loss: 0.2633 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.88it/s]\n",
            "BLEU: 0.1427, loss: 4.0881 ||: 100%|██████████| 147/147 [00:13<00:00, 11.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*******************************Completed 70 epochs***********************************\n",
            "SOURCE: [@start@, I, have, to, go, to, sleep, ., @end@]\n",
            "GOLD: [@start@, 我, 该, 去, 睡, 觉, 了, 。, @end@]\n",
            "PRED: ['我', '睡', '觉', '了', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, just, do, n't, know, what, to, say, ., @end@]\n",
            "GOLD: [@start@, 我, 就, 是, 不, 知, 道, 說, 些, 什, 麼, 。, @end@]\n",
            "PRED: ['我', '只', '是', '不', '知', '道', '该', '说', '什', '么', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, may, give, up, soon, and, just, nap, instead, ., @end@]\n",
            "GOLD: [@start@, 也, 许, 我, 会, 马, 上, 放, 弃, 然, 后, 去, 睡, 一, 觉, 。, @end@]\n",
            "PRED: ['我', '一', '无', '四', '時', '可', '能', '早', '點', '兒', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, 'm, going, to, go, ., @end@]\n",
            "GOLD: [@start@, 我, 要, 走, 了, 。, @end@]\n",
            "PRED: ['我', '要', '去', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, That, 's, MY, line, !, @end@]\n",
            "GOLD: [@start@, 那, 是, 我, 的, 台, 词, ！, @end@]\n",
            "PRED: ['那', '是', '我', '該', '說', '的', '話', '!']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, It, does, n't, surprise, me, ., @end@]\n",
            "GOLD: [@start@, 这, 并, 不, 让, 我, 惊, 讶, 。, @end@]\n",
            "PRED: ['它', '讓', '我', '嚇', '到', '了', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, 'm, not, a, real, fish, ,, I, 'm, just, a, mere, plushy, ., @end@]\n",
            "GOLD: [@start@, 我, 不, 是, 一, 条, 真, 的, 鱼, ，, 我, 只, 是, 一, 个, 长, 毛, 绒, 玩, 具, 。, @end@]\n",
            "PRED: ['我', '不', '是', '这', '么', '贵', ',', ' ', '但', '我', '还', '是', '一', '无', '所', '知', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, thought, you, liked, to, learn, new, things, ., @end@]\n",
            "GOLD: [@start@, 我, 以, 為, 你, 喜, 歡, 學, 習, 新, 事, 物, 。, @end@]\n",
            "PRED: ['我', '以', '为', '你', '新', '学', '习', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, This, is, not, important, ., @end@]\n",
            "GOLD: [@start@, 這, 個, 不, 重, 要, 。, @end@]\n",
            "PRED: ['这', '不', '重', '要', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, Thanks, for, having, explained, to, me, at, last, why, people, take, me, for, an, idiot, ., @end@]\n",
            "GOLD: [@start@, 感, 谢, 最, 后, 为, 我, 说, 明, 了, 为, 什, 么, 人, 们, 把, 我, 当, 作, 傻, 瓜, 了, 。, @end@]\n",
            "PRED: ['谢', '谢', '你', '听', '到', '老', '婆', '和', '父', '母', '告', '诉', '你', '。']\n",
            "////////////////////////////////////////////\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlLz7aRsHRBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "outputId": "de7ffd7a-62f3-41ee-9a5d-111f497cf7c9"
      },
      "source": [
        "vocab2 = Vocabulary.from_files(\"/content/drive/My Drive/Eng_Mandarin/data/tmp_v02.1/vocabulary\")\n",
        "model2 = SimpleSeq2Seq(vocab2, source_embedder, encoder, max_decoding_steps,\n",
        "                      target_embedding_dim=ZH_EMBEDDING_DIM,\n",
        "                      target_namespace='target_tokens',\n",
        "                      attention=attention,\n",
        "                      beam_size=8,\n",
        "                      use_bleu=True)\n",
        "\n",
        "def convert(s): \n",
        "    new = \"\"   \n",
        "    for x in s: \n",
        "        new += x   \n",
        "    return new \n",
        "\n",
        "with open(\"/content/drive/My Drive/Eng_Mandarin/data/tmp_v02.1/model.th\", 'rb') as f:\n",
        "  model2.load_state_dict(torch.load(f))\n",
        "\n",
        "if CUDA_DEVICE > -1:\n",
        "    model2.cuda(CUDA_DEVICE)\n",
        "\n",
        "predictor2 = SimpleSeq2SeqPredictor(model2, reader)\n",
        "\n",
        "for instance in itertools.islice(validation_dataset, 10):\n",
        "  print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "  print('GOLD:', instance.fields['target_tokens'].tokens)\n",
        "  print('PRED:', convert(predictor2.predict_instance(instance)['predicted_tokens']))\n",
        "  #print('PRED:', predictor2.predict_instance(instance)['predicted_tokens'])\n",
        "  print('////////////////////////////////////////////\\n')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SOURCE: [@start@, I, have, to, go, to, sleep, ., @end@]\n",
            "GOLD: [@start@, 我, 该, 去, 睡, 觉, 了, 。, @end@]\n",
            "PRED: 我该睡睡觉。。\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, just, do, n't, know, what, to, say, ., @end@]\n",
            "GOLD: [@start@, 我, 就, 是, 不, 知, 道, 說, 些, 什, 麼, 。, @end@]\n",
            "PRED: 我只是不知道本什什么。\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, may, give, up, soon, and, just, nap, instead, ., @end@]\n",
            "GOLD: [@start@, 也, 许, 我, 会, 马, 上, 放, 弃, 然, 后, 去, 睡, 一, 觉, 。, @end@]\n",
            "PRED: 我许我们早上离下。后。。觉下。\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, 'm, going, to, go, ., @end@]\n",
            "GOLD: [@start@, 我, 要, 走, 了, 。, @end@]\n",
            "PRED: 我要去了。\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, That, 's, MY, line, !, @end@]\n",
            "GOLD: [@start@, 那, 是, 我, 的, 台, 词, ！, @end@]\n",
            "PRED: 那是我該环程！\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, It, does, n't, surprise, me, ., @end@]\n",
            "GOLD: [@start@, 这, 并, 不, 让, 我, 惊, 讶, 。, @end@]\n",
            "PRED: 它个没打我惊讶。\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, 'm, not, a, real, fish, ,, I, 'm, just, a, mere, plushy, ., @end@]\n",
            "GOLD: [@start@, 我, 不, 是, 一, 条, 真, 的, 鱼, ，, 我, 只, 是, 一, 个, 长, 毛, 绒, 玩, 具, 。, @end@]\n",
            "PRED: 我不是这笔新正呆，就还是@@UNKNOWN@@@@UNKNOWN@@@@UNKNOWN@@的前的的。\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, thought, you, liked, to, learn, new, things, ., @end@]\n",
            "GOLD: [@start@, 我, 以, 為, 你, 喜, 歡, 學, 習, 新, 事, 物, 。, @end@]\n",
            "PRED: 我想为你喜歡哪家漢的。。\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, This, is, not, important, ., @end@]\n",
            "GOLD: [@start@, 這, 個, 不, 重, 要, 。, @end@]\n",
            "PRED: 这不不重要。\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, Thanks, for, having, explained, to, me, at, last, why, people, take, me, for, an, idiot, ., @end@]\n",
            "GOLD: [@start@, 感, 谢, 最, 后, 为, 我, 说, 明, 了, 为, 什, 么, 人, 们, 把, 我, 当, 作, 傻, 瓜, 了, 。, @end@]\n",
            "PRED: 谢谢你好告我告明“你什么叫帮对这说做这一解解\n",
            "////////////////////////////////////////////\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}