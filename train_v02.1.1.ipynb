{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_v02.1.1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RtjShreyD/Eng-Mandarin/blob/master/train_v02.1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I48m98tYAnhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6b17c6ac-908e-4bb7-ae99-45c33fd55c70"
      },
      "source": [
        "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.3.0.post4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJg8OGXVBdsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "db6b61c4-9fcd-4def-f201-3af11f50b261"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Collecting torch==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6MB 25kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 0.3.0.post4\n",
            "    Uninstalling torch-0.3.0.post4:\n",
            "      Successfully uninstalled torch-0.3.0.post4\n",
            "Successfully installed torch-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YLIWneIB2KJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "45f8ec22-f490-43fa-d704-3b5556c8569b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHD2dwOBCLmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3c01e0e-7ae8-47ae-f2eb-7f482bbde30d"
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 20.1MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 71.3MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 57.6MB/s \n",
            "\u001b[?25hCollecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.3)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.40)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.4)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 63.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 58.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from responses>=0.7->allennlp) (1.12.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.9)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.40)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Building wheels for collected packages: overrides, numpydoc, parsimonious, ftfy, word2number, jsonnet\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp36-none-any.whl size=5600 sha256=46a0ea051af30debf88b32f1550a44bc4501448321cf20f88fa1c4009981ba0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31894 sha256=adfb8c977911b55ac78605ac955e0f7f6df3bbbec070a3e3e9bed234068af037\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=356a188d3e30e00d59f29b2a3a1b4da69b53da4e705c32ddeaec0e6dfd8ae88a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=c3e73b2a2eb09f018008556e78973014504d853b7216ce7b20c98e19b855e7c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=8069c470ac823cf480d87ccd5f7c3d2317a36aaca2cd05e5938946f6199bfaf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320348 sha256=700e5938cfe7c1c9d6a3955a62a456a1458cfde2cf866123b440bfd7dd33b5e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "Successfully built overrides numpydoc parsimonious ftfy word2number jsonnet\n",
            "Installing collected packages: overrides, responses, numpydoc, jsonpickle, pytorch-pretrained-bert, tensorboardX, flaky, unidecode, sentencepiece, pytorch-transformers, flask-cors, parsimonious, ftfy, word2number, conllu, jsonnet, allennlp\n",
            "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.2 overrides-2.7.0 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 sentencepiece-0.1.85 tensorboardX-1.9 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiKHWg-8Cr0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.dataset_readers.seq2seq import Seq2SeqDatasetReader\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
        "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.nn.activations import Activation\n",
        "from allennlp.models.encoder_decoders.simple_seq2seq import SimpleSeq2Seq\n",
        "from allennlp.modules.attention import LinearAttention, BilinearAttention, DotProductAttention\n",
        "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, StackedSelfAttentionEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.predictors import SimpleSeq2SeqPredictor\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "EN_EMBEDDING_DIM = 256\n",
        "ZH_EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT_pNvy6GCeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c68e92d9-8015-4292-8bd2-4fde01d55bba"
      },
      "source": [
        "reader = Seq2SeqDatasetReader(\n",
        "        source_tokenizer=WordTokenizer(),\n",
        "        target_tokenizer=CharacterTokenizer(),\n",
        "        source_token_indexers={'tokens': SingleIdTokenIndexer()},\n",
        "        target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\n",
        "    \n",
        "train_dataset = reader.read('/content/drive/My Drive/Eng_Mandarin/data/tatoeba.eng_cmn.train.tsv')\n",
        "validation_dataset = reader.read('/content/drive/My Drive/Eng_Mandarin/data/tatoeba.eng_cmn.dev.tsv')\n",
        "\n",
        "vocab = Vocabulary.from_instances(train_dataset + validation_dataset, \n",
        "                                  min_count={'tokens': 3, 'target_tokens': 3})\n",
        "\n",
        "en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), \n",
        "                         embedding_dim=EN_EMBEDDING_DIM)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37400it [00:13, 2848.79it/s]\n",
            "4676it [00:01, 3000.06it/s]\n",
            "100%|██████████| 42076/42076 [00:00<00:00, 65935.35it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkKjDzNNGKQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = StackedSelfAttentionEncoder(\n",
        "        input_dim=EN_EMBEDDING_DIM, \n",
        "        hidden_dim=HIDDEN_DIM, \n",
        "        projection_dim=128, \n",
        "        feedforward_hidden_dim=128, \n",
        "        num_layers=1, \n",
        "        num_attention_heads=8)\n",
        "\n",
        "source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBAAOxIyGWjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56f01ec2-6934-4be6-93c8-484efbeedbea"
      },
      "source": [
        "attention = DotProductAttention()\n",
        "\n",
        "max_decoding_steps = 20 \n",
        "\n",
        "model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
        "                      target_embedding_dim=ZH_EMBEDDING_DIM,\n",
        "                      target_namespace='target_tokens',\n",
        "                      attention=attention,\n",
        "                      beam_size=8,\n",
        "                      use_bleu=True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  CUDA_DEVICE = 0\n",
        "  model = model.cuda(CUDA_DEVICE)\n",
        "  print(\"Model deployed on GPU\")\n",
        "else:\n",
        "  CUDA_DEVICE = -1\n",
        "  print(\"Model deployed on CPU\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
        "\n",
        "iterator.index_with(vocab)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model deployed on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4yZwOfjvUw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2cf6279-dbff-4e09-fd96-850e00ca2feb"
      },
      "source": [
        "mod_path = '/content/drive/My Drive/Eng_Mandarin/data/tmp_v02.1'\n",
        "try:   \n",
        "    if(os.path.exists(mod_path)):\n",
        "      print(\"\\nModel Path exists move ahead\")\n",
        "    else:\n",
        "      os.mkdir(mod_path)\n",
        "except OSError:\n",
        "    print (\"Creation of the directory failed\")\n",
        "else:  \n",
        "    print (\"Successfully created the tmp directory model path\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the tmp directory model path\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ42Pm9XrzZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "307b7def-1aa4-48ee-b305-41bc838b8faa"
      },
      "source": [
        "try:\n",
        "  trainer = Trainer(model=model, \n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset,\n",
        "                  num_epochs=50,\n",
        "                  cuda_device=CUDA_DEVICE)\n",
        "                     \n",
        "  trainer.train()\n",
        "  print(\"*******************************Completed 50 epochs***********************************\")\n",
        "  predictor = SimpleSeq2SeqPredictor(model, reader)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"Training interrupted....all weights saved\")\n",
        "\n",
        "for instance in itertools.islice(validation_dataset, 10):\n",
        "  print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "  print('GOLD:', instance.fields['target_tokens'].tokens)\n",
        "  print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])\n",
        "  print('////////////////////////////////////////////\\n')\n",
        "\n",
        "with open(os.path.join(mod_path, \"model.th\"), 'wb') as f:\n",
        "    torch.save(model.state_dict(), f)\n",
        "vocab.save_to_files(os.path.join(mod_path, \"vocabulary\"))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:allennlp.training.trainer:You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "loss: 4.6796 ||: 100%|██████████| 1169/1169 [00:39<00:00, 29.41it/s]\n",
            "BLEU: 0.0092, loss: 4.0981 ||: 100%|██████████| 147/147 [00:12<00:00, 12.23it/s]\n",
            "loss: 3.7815 ||: 100%|██████████| 1169/1169 [00:37<00:00, 24.16it/s]\n",
            "BLEU: 0.0191, loss: 3.6414 ||: 100%|██████████| 147/147 [00:13<00:00, 11.08it/s]\n",
            "loss: 3.3660 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.20it/s]\n",
            "BLEU: 0.0246, loss: 3.3702 ||: 100%|██████████| 147/147 [00:13<00:00, 11.25it/s]\n",
            "loss: 3.0681 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.49it/s]\n",
            "BLEU: 0.0341, loss: 3.1733 ||: 100%|██████████| 147/147 [00:13<00:00, 11.14it/s]\n",
            "loss: 2.8224 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.56it/s]\n",
            "BLEU: 0.0457, loss: 3.0186 ||: 100%|██████████| 147/147 [00:12<00:00, 11.32it/s]\n",
            "loss: 2.6086 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.59it/s]\n",
            "BLEU: 0.0578, loss: 2.8986 ||: 100%|██████████| 147/147 [00:13<00:00, 11.06it/s]\n",
            "loss: 2.4173 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.63it/s]\n",
            "BLEU: 0.0671, loss: 2.8027 ||: 100%|██████████| 147/147 [00:13<00:00, 11.18it/s]\n",
            "loss: 2.2516 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.44it/s]\n",
            "BLEU: 0.0744, loss: 2.7353 ||: 100%|██████████| 147/147 [00:13<00:00, 11.24it/s]\n",
            "loss: 2.1043 ||: 100%|██████████| 1169/1169 [00:37<00:00, 33.23it/s]\n",
            "BLEU: 0.0836, loss: 2.6813 ||: 100%|██████████| 147/147 [00:13<00:00, 11.07it/s]\n",
            "loss: 1.9751 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.76it/s]\n",
            "BLEU: 0.0907, loss: 2.6454 ||: 100%|██████████| 147/147 [00:12<00:00, 11.39it/s]\n",
            "loss: 1.8564 ||: 100%|██████████| 1169/1169 [00:37<00:00, 33.70it/s]\n",
            "BLEU: 0.0957, loss: 2.6218 ||: 100%|██████████| 147/147 [00:13<00:00, 11.26it/s]\n",
            "loss: 1.7537 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.49it/s]\n",
            "BLEU: 0.1050, loss: 2.6130 ||: 100%|██████████| 147/147 [00:13<00:00, 11.04it/s]\n",
            "loss: 1.6577 ||: 100%|██████████| 1169/1169 [00:37<00:00, 27.27it/s]\n",
            "BLEU: 0.1039, loss: 2.5914 ||: 100%|██████████| 147/147 [00:13<00:00,  7.21it/s]\n",
            "loss: 1.5709 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.55it/s]\n",
            "BLEU: 0.1112, loss: 2.5955 ||: 100%|██████████| 147/147 [00:13<00:00, 10.93it/s]\n",
            "loss: 1.4908 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.20it/s]\n",
            "BLEU: 0.1175, loss: 2.6037 ||: 100%|██████████| 147/147 [00:13<00:00, 11.08it/s]\n",
            "loss: 1.4157 ||: 100%|██████████| 1169/1169 [00:37<00:00, 30.80it/s]\n",
            "BLEU: 0.1160, loss: 2.6242 ||: 100%|██████████| 147/147 [00:13<00:00, 11.09it/s]\n",
            "loss: 1.3476 ||: 100%|██████████| 1169/1169 [00:37<00:00, 32.29it/s]\n",
            "BLEU: 0.1188, loss: 2.6335 ||: 100%|██████████| 147/147 [00:13<00:00, 11.15it/s]\n",
            "loss: 1.2838 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.37it/s]\n",
            "BLEU: 0.1189, loss: 2.6508 ||: 100%|██████████| 147/147 [00:13<00:00, 11.21it/s]\n",
            "loss: 1.2252 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.65it/s]\n",
            "BLEU: 0.1228, loss: 2.6587 ||: 100%|██████████| 147/147 [00:13<00:00, 11.18it/s]\n",
            "loss: 1.1682 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.55it/s]\n",
            "BLEU: 0.1287, loss: 2.6951 ||: 100%|██████████| 147/147 [00:13<00:00, 11.19it/s]\n",
            "loss: 1.1174 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.70it/s]\n",
            "BLEU: 0.1278, loss: 2.7169 ||: 100%|██████████| 147/147 [00:13<00:00, 11.17it/s]\n",
            "loss: 1.0695 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.05it/s]\n",
            "BLEU: 0.1261, loss: 2.7523 ||: 100%|██████████| 147/147 [00:12<00:00,  7.25it/s]\n",
            "loss: 1.0229 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.21it/s]\n",
            "BLEU: 0.1287, loss: 2.7731 ||: 100%|██████████| 147/147 [00:12<00:00, 11.33it/s]\n",
            "loss: 0.9799 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.84it/s]\n",
            "BLEU: 0.1344, loss: 2.7988 ||: 100%|██████████| 147/147 [00:13<00:00, 11.19it/s]\n",
            "loss: 0.9390 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.67it/s]\n",
            "BLEU: 0.1321, loss: 2.8357 ||: 100%|██████████| 147/147 [00:13<00:00,  7.17it/s]\n",
            "loss: 0.9023 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.88it/s]\n",
            "BLEU: 0.1305, loss: 2.8481 ||: 100%|██████████| 147/147 [00:13<00:00, 11.03it/s]\n",
            "loss: 0.8667 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.84it/s]\n",
            "BLEU: 0.1323, loss: 2.8980 ||: 100%|██████████| 147/147 [00:13<00:00, 11.23it/s]\n",
            "loss: 0.8315 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.82it/s]\n",
            "BLEU: 0.1356, loss: 2.9202 ||: 100%|██████████| 147/147 [00:13<00:00,  6.95it/s]\n",
            "loss: 0.8010 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.25it/s]\n",
            "BLEU: 0.1350, loss: 2.9485 ||: 100%|██████████| 147/147 [00:13<00:00, 11.05it/s]\n",
            "loss: 0.7693 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.80it/s]\n",
            "BLEU: 0.1350, loss: 2.9981 ||: 100%|██████████| 147/147 [00:13<00:00, 11.20it/s]\n",
            "loss: 0.7423 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.89it/s]\n",
            "BLEU: 0.1350, loss: 3.0239 ||: 100%|██████████| 147/147 [00:13<00:00, 11.19it/s]\n",
            "loss: 0.7149 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.93it/s]\n",
            "BLEU: 0.1355, loss: 3.0592 ||: 100%|██████████| 147/147 [00:13<00:00, 11.20it/s]\n",
            "loss: 0.6905 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.93it/s]\n",
            "BLEU: 0.1367, loss: 3.0826 ||: 100%|██████████| 147/147 [00:13<00:00, 11.20it/s]\n",
            "loss: 0.6662 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.69it/s]\n",
            "BLEU: 0.1363, loss: 3.1277 ||: 100%|██████████| 147/147 [00:13<00:00, 11.25it/s]\n",
            "loss: 0.6429 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.38it/s]\n",
            "BLEU: 0.1343, loss: 3.1568 ||: 100%|██████████| 147/147 [00:13<00:00, 11.03it/s]\n",
            "loss: 0.6202 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.04it/s]\n",
            "BLEU: 0.1399, loss: 3.1973 ||: 100%|██████████| 147/147 [00:13<00:00, 10.97it/s]\n",
            "loss: 0.6022 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.96it/s]\n",
            "BLEU: 0.1383, loss: 3.2241 ||: 100%|██████████| 147/147 [00:13<00:00, 11.22it/s]\n",
            "loss: 0.5818 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.97it/s]\n",
            "BLEU: 0.1351, loss: 3.2490 ||: 100%|██████████| 147/147 [00:13<00:00, 11.20it/s]\n",
            "loss: 0.5643 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.95it/s]\n",
            "BLEU: 0.1397, loss: 3.2955 ||: 100%|██████████| 147/147 [00:13<00:00, 11.11it/s]\n",
            "loss: 0.5465 ||: 100%|██████████| 1169/1169 [00:36<00:00, 34.54it/s]\n",
            "BLEU: 0.1367, loss: 3.3187 ||: 100%|██████████| 147/147 [00:13<00:00,  7.13it/s]\n",
            "loss: 0.5298 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.69it/s]\n",
            "BLEU: 0.1377, loss: 3.3417 ||: 100%|██████████| 147/147 [00:13<00:00,  7.16it/s]\n",
            "loss: 0.5136 ||: 100%|██████████| 1169/1169 [00:37<00:00, 30.49it/s]\n",
            "BLEU: 0.1376, loss: 3.3866 ||: 100%|██████████| 147/147 [00:13<00:00, 11.23it/s]\n",
            "loss: 0.4990 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.83it/s]\n",
            "BLEU: 0.1369, loss: 3.4185 ||: 100%|██████████| 147/147 [00:13<00:00, 11.02it/s]\n",
            "loss: 0.4843 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.72it/s]\n",
            "BLEU: 0.1378, loss: 3.4621 ||: 100%|██████████| 147/147 [00:13<00:00, 11.06it/s]\n",
            "loss: 0.4718 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.87it/s]\n",
            "BLEU: 0.1356, loss: 3.4850 ||: 100%|██████████| 147/147 [00:13<00:00, 11.28it/s]\n",
            "loss: 0.4602 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.58it/s]\n",
            "BLEU: 0.1385, loss: 3.5122 ||: 100%|██████████| 147/147 [00:13<00:00, 11.00it/s]\n",
            "loss: 0.4478 ||: 100%|██████████| 1169/1169 [00:36<00:00, 31.87it/s]\n",
            "BLEU: 0.1343, loss: 3.5543 ||: 100%|██████████| 147/147 [00:13<00:00, 11.11it/s]\n",
            "loss: 0.4346 ||: 100%|██████████| 1169/1169 [00:37<00:00, 31.39it/s]\n",
            "BLEU: 0.1373, loss: 3.5764 ||: 100%|██████████| 147/147 [00:13<00:00, 11.13it/s]\n",
            "loss: 0.4240 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.05it/s]\n",
            "BLEU: 0.1362, loss: 3.6148 ||: 100%|██████████| 147/147 [00:13<00:00, 11.16it/s]\n",
            "loss: 0.4130 ||: 100%|██████████| 1169/1169 [00:36<00:00, 32.00it/s]\n",
            "BLEU: 0.1397, loss: 3.6330 ||: 100%|██████████| 147/147 [00:13<00:00, 11.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*******************************Completed 50 epochs***********************************\n",
            "SOURCE: [@start@, I, have, to, go, to, sleep, ., @end@]\n",
            "GOLD: [@start@, 我, 该, 去, 睡, 觉, 了, 。, @end@]\n",
            "PRED: ['我', '该', '睡', '觉', '了', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, just, do, n't, know, what, to, say, ., @end@]\n",
            "GOLD: [@start@, 我, 就, 是, 不, 知, 道, 說, 些, 什, 麼, 。, @end@]\n",
            "PRED: ['我', '就', '知', '道', '怎', '么', '做', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, may, give, up, soon, and, just, nap, instead, ., @end@]\n",
            "GOLD: [@start@, 也, 许, 我, 会, 马, 上, 放, 弃, 然, 后, 去, 睡, 一, 觉, 。, @end@]\n",
            "PRED: ['我', '尽', '快', '带', '大', '家', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, 'm, going, to, go, ., @end@]\n",
            "GOLD: [@start@, 我, 要, 走, 了, 。, @end@]\n",
            "PRED: ['我', '去', '去', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, That, 's, MY, line, !, @end@]\n",
            "GOLD: [@start@, 那, 是, 我, 的, 台, 词, ！, @end@]\n",
            "PRED: ['那', '是', '我', '該', '說', '的', '話', '!']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, It, does, n't, surprise, me, ., @end@]\n",
            "GOLD: [@start@, 这, 并, 不, 让, 我, 惊, 讶, 。, @end@]\n",
            "PRED: ['它', '不', '給', '我', '了', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, 'm, not, a, real, fish, ,, I, 'm, just, a, mere, plushy, ., @end@]\n",
            "GOLD: [@start@, 我, 不, 是, 一, 条, 真, 的, 鱼, ，, 我, 只, 是, 一, 个, 长, 毛, 绒, 玩, 具, 。, @end@]\n",
            "PRED: ['我', '不', '是', '本', '质', '學', '的', '，', '但', '是', '我', '還', '會', '有', '一', '種', '牛', '奶', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, thought, you, liked, to, learn, new, things, ., @end@]\n",
            "GOLD: [@start@, 我, 以, 為, 你, 喜, 歡, 學, 習, 新, 事, 物, 。, @end@]\n",
            "PRED: ['我', '认', '为', '你', '新', '学', '习', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, This, is, not, important, ., @end@]\n",
            "GOLD: [@start@, 這, 個, 不, 重, 要, 。, @end@]\n",
            "PRED: ['这', '不', '重', '要', '。']\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, Thanks, for, having, explained, to, me, at, last, why, people, take, me, for, an, idiot, ., @end@]\n",
            "GOLD: [@start@, 感, 谢, 最, 后, 为, 我, 说, 明, 了, 为, 什, 么, 人, 们, 把, 我, 当, 作, 傻, 瓜, 了, 。, @end@]\n",
            "PRED: ['去', '意', '思', '考', '试', '如', '说', '什', '么', '？', '我', '很', '犹', '豫', '我', '可', '以', '告', '诉', '我']\n",
            "////////////////////////////////////////////\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7a32gTwhPog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "f7416347-1d91-4ad8-898a-49a96a60a5e4"
      },
      "source": [
        "!pip install googletrans"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/f0/a22d41d3846d1f46a4f20086141e0428ccc9c6d644aacbfd30990cf46886/googletrans-2.4.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.8)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-2.4.0-cp36-none-any.whl size=15776 sha256=12ae58dac7c5e1a6ce2f5c3643a300ab38d2a8204bfb8b5e3c19a0e0def517fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/d6/e7/a8efd5f2427d5eb258070048718fa56ee5ac57fd6f53505f95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: googletrans\n",
            "Successfully installed googletrans-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFKTSkbthUNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szGZi-zrhlrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(s): \n",
        "       \n",
        "    new = ''.join(s)\n",
        "    return new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlLz7aRsHRBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e12d5d31-c6b6-4622-bd50-e0dbfe2d614f"
      },
      "source": [
        "vocab2 = Vocabulary.from_files(\"/content/drive/My Drive/Eng_Mandarin/data/tmp_v02.1/vocabulary\")\n",
        "model2 = SimpleSeq2Seq(vocab2, source_embedder, encoder, max_decoding_steps,\n",
        "                      target_embedding_dim=ZH_EMBEDDING_DIM,\n",
        "                      target_namespace='target_tokens',\n",
        "                      attention=attention,\n",
        "                      beam_size=8,\n",
        "                      use_bleu=True)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/My Drive/Eng_Mandarin/data/tmp_v02.1/model.th\", 'rb') as f:\n",
        "  model2.load_state_dict(torch.load(f))\n",
        "\n",
        "if CUDA_DEVICE > -1:\n",
        "    model2.cuda(CUDA_DEVICE)\n",
        "\n",
        "predictor2 = SimpleSeq2SeqPredictor(model2, reader)\n",
        "\n",
        "for instance in itertools.islice(validation_dataset, 10):\n",
        "  print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
        "  print('GOLD:', instance.fields['target_tokens'].tokens)\n",
        "\n",
        "  pred_str = predictor2.predict_instance(instance)['predicted_tokens']\n",
        "  translated = translator.translate(pred_str) \n",
        "  print('PRED:', pred_str)\n",
        "  print('GOOGLE translation of predicted string is below')\n",
        "  print(translated.text)\n",
        "  print('////////////////////////////////////////////\\n')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SOURCE: [@start@, I, have, to, go, to, sleep, ., @end@]\n",
            "GOLD: [@start@, 我, 该, 去, 睡, 觉, 了, 。, @end@]\n",
            "PRED: 我该睡睡觉了。\n",
            "GOOGLE translation of predicted string is below\n",
            "I slept the sleep.\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, just, do, n't, know, what, to, say, ., @end@]\n",
            "GOLD: [@start@, 我, 就, 是, 不, 知, 道, 說, 些, 什, 麼, 。, @end@]\n",
            "PRED: 我就知不知道该什什麼。\n",
            "GOOGLE translation of predicted string is below\n",
            "I know what the what.\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, may, give, up, soon, and, just, nap, instead, ., @end@]\n",
            "GOLD: [@start@, 也, 许, 我, 会, 马, 上, 放, 弃, 然, 后, 去, 睡, 一, 觉, 。, @end@]\n",
            "PRED: 我許我很那上，去而后变。。步。\n",
            "GOOGLE translation of predicted string is below\n",
            "I promise that I'm on, then go change. . step.\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, 'm, going, to, go, ., @end@]\n",
            "GOLD: [@start@, 我, 要, 走, 了, 。, @end@]\n",
            "PRED: 我去去了。\n",
            "GOOGLE translation of predicted string is below\n",
            "I'm gonna go up.\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, That, 's, MY, line, !, @end@]\n",
            "GOLD: [@start@, 那, 是, 我, 的, 台, 词, ！, @end@]\n",
            "PRED: 那是我該想光!\n",
            "GOOGLE translation of predicted string is below\n",
            "I think that's the light!\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, It, does, n't, surprise, me, ., @end@]\n",
            "GOLD: [@start@, 这, 并, 不, 让, 我, 惊, 讶, 。, @end@]\n",
            "PRED: 它不不谢我发讶。\n",
            "GOOGLE translation of predicted string is below\n",
            "It does not thank me surprised.\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, 'm, not, a, real, fish, ,, I, 'm, just, a, mere, plushy, ., @end@]\n",
            "GOLD: [@start@, 我, 不, 是, 一, 条, 真, 的, 鱼, ，, 我, 只, 是, 一, 个, 长, 毛, 绒, 玩, 具, 。, @end@]\n",
            "PRED: 我不是本本本正，，而不是3个花日字，的，\n",
            "GOOGLE translation of predicted string is below\n",
            "I'm not positive this books ,, instead of three flowers on the word, of,\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, I, thought, you, liked, to, learn, new, things, ., @end@]\n",
            "GOLD: [@start@, 我, 以, 為, 你, 喜, 歡, 學, 習, 新, 事, 物, 。, @end@]\n",
            "PRED: 我想为你喜歡這識新的了。\n",
            "GOOGLE translation of predicted string is below\n",
            "I want to know you like this new one.\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, This, is, not, important, ., @end@]\n",
            "GOLD: [@start@, 這, 個, 不, 重, 要, 。, @end@]\n",
            "PRED: 這個不重要。\n",
            "GOOGLE translation of predicted string is below\n",
            "This is not important.\n",
            "////////////////////////////////////////////\n",
            "\n",
            "SOURCE: [@start@, Thanks, for, having, explained, to, me, at, last, why, people, take, me, for, an, idiot, ., @end@]\n",
            "GOLD: [@start@, 感, 谢, 最, 后, 为, 我, 说, 明, 了, 为, 什, 么, 人, 们, 把, 我, 当, 作, 傻, 瓜, 了, 。, @end@]\n",
            "PRED: 去谢为后一什说话了什了么不需需我所上黑向。。我\n",
            "GOOGLE translation of predicted string is below\n",
            "Xie is a go after even begin to speak what I do not need to be on the black. . I\n",
            "////////////////////////////////////////////\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}